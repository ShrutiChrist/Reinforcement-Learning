{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrutiChrist/Reinforcement-Learning/blob/main/LAB6_RL_decision_making.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning environment to support model-based algorithms for decision-making."
      ],
      "metadata": {
        "id": "GCX7L17a9XxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaOcXRFqlJle"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelBasedGridWorld(gym.Env):\n",
        "    def __init__(self, grid_size=5, terminal_states=None, random_seed=None):\n",
        "        \"\"\"\n",
        "        Custom GridWorld environment for model-based reinforcement learning.\n",
        "        :param grid_size: Size of the grid (grid_size x grid_size).\n",
        "        :param terminal_states: List of terminal state positions (row, col).\n",
        "        :param random_seed: Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.terminal_states = terminal_states or [(grid_size - 1, grid_size - 1)]\n",
        "        self.random_seed = random_seed\n",
        "        self._setup_environment()\n",
        "\n",
        "    def _setup_environment(self):\n",
        "        # Initialize state and action spaces\n",
        "        self.action_space = spaces.Discrete(4)  # Actions: 0=Up, 1=Right, 2=Down, 3=Left\n",
        "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size])\n",
        "        self.state = (0, 0)  # Start at top-left corner\n",
        "        self.reward_model = {}  # Reward function R(s, a)\n",
        "        self.transition_model = {}  # Transition dynamics P(s'|s, a)\n",
        "        if self.random_seed:\n",
        "            np.random.seed(self.random_seed)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take an action and return the next state, reward, done, and info.\"\"\"\n",
        "        if action < 0 or action >= self.action_space.n:\n",
        "            raise ValueError(\"Invalid action.\")\n",
        "\n",
        "        row, col = self.state\n",
        "        if self.state in self.terminal_states:\n",
        "            return self.state, 0, True, {}\n",
        "\n",
        "        # Define movement directions\n",
        "        moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}  # Up, Right, Down, Left\n",
        "        dr, dc = moves[action]\n",
        "        next_row, next_col = row + dr, col + dc\n",
        "\n",
        "        # Ensure the next state stays within bounds\n",
        "        next_row = max(0, min(self.grid_size - 1, next_row))\n",
        "        next_col = max(0, min(self.grid_size - 1, next_col))\n",
        "        next_state = (next_row, next_col)\n",
        "\n",
        "        # Reward is -1 for each step unless in terminal state\n",
        "        reward = -1 if next_state not in self.terminal_states else 10\n",
        "        done = next_state in self.terminal_states\n",
        "\n",
        "        # Update state\n",
        "        self.state = next_state\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the current state of the environment.\"\"\"\n",
        "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
        "        grid[:, :] = '.'\n",
        "        for r, c in self.terminal_states:\n",
        "            grid[r, c] = 'T'  # Mark terminal states\n",
        "        row, col = self.state\n",
        "        grid[row, col] = 'A'  # Mark agent position\n",
        "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
        "        print()\n",
        "\n",
        "    def get_transition_model(self):\n",
        "        \"\"\"Return the transition dynamics for model-based RL.\"\"\"\n",
        "        return self.transition_model\n",
        "\n",
        "    def get_reward_model(self):\n",
        "        \"\"\"Return the reward function for model-based RL.\"\"\"\n",
        "        return self.reward_model"
      ],
      "metadata": {
        "id": "ZP-Xu1biwKwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment\n",
        "env = ModelBasedGridWorld(grid_size=5, terminal_states=[(4, 4)])\n",
        "\n",
        "# Reset the environment\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "# Simulate a random episode\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # Random action\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    env.render()\n",
        "    print(f\"Action: {action}, Reward: {reward}\")"
      ],
      "metadata": {
        "id": "KlJ-lBcYwKyK",
        "outputId": "7ba0def1-40c3-4dab-8f60-015e27402d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 0, Reward: -1\n",
            ". . . . .\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 0, Reward: -1\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 0, Reward: -1\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . . . .\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . A\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 0, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 0, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A T\n",
            "\n",
            "Action: 2, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . T\n",
            "\n",
            "Action: 3, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A T\n",
            "\n",
            "Action: 1, Reward: -1\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . A\n",
            "\n",
            "Action: 1, Reward: 10\n"
          ]
        }
      ]
    }
  ]
}